{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is for following along (and modifying and testing) the code as shown in the book \"Blueprints for Text Analytics Using Python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"It was the best of times\", \"it was the worst of times\", \"it was the age of wisdom\", \n",
    "            \"it was the age of foolishness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [[t for t in sentence.split()] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['It', 'was', 'the', 'best', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'worst', 'of', 'times'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'wisdom'],\n",
       " ['it', 'was', 'the', 'age', 'of', 'foolishness']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set([w for s in tokenized_sentences for w in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'It',\n",
       " 'age',\n",
       " 'best',\n",
       " 'foolishness',\n",
       " 'it',\n",
       " 'of',\n",
       " 'the',\n",
       " 'times',\n",
       " 'was',\n",
       " 'wisdom',\n",
       " 'worst'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>times</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>was</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>of</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>worst</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>foolishness</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wisdom</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   1\n",
       "0           age   0\n",
       "1         times   1\n",
       "2            it   2\n",
       "3           the   3\n",
       "4          best   4\n",
       "5           was   5\n",
       "6            of   6\n",
       "7         worst   7\n",
       "8   foolishness   8\n",
       "9            It   9\n",
       "10       wisdom  10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[w, i] for i, w in enumerate(vocabulary)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to now do one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(tokenized_sentence):\n",
    "    return [1 if w in tokenized_sentence else 0 for w in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = [onehot_encode(tokenized_sentence) for tokenized_sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0],\n",
       " [0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0],\n",
       " [1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1],\n",
       " [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0]: It was the best of times\n",
      "[0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]: it was the worst of times\n",
      "[1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]: it was the age of wisdom\n",
      "[1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]: it was the age of foolishness\n"
     ]
    }
   ],
   "source": [
    "for (sentence, oh) in zip(sentences, onehot):\n",
    "    print(\"%s: %s\" % (oh, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if introduce a document not in here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1], 'the age of wisdom is the best of times')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encode(\"the age of wisdom is the best of times\".split()), \"the age of wisdom is the best of times\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 'Is the football season now maybe')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if no word in the vocabulary is shown\n",
    "onehot_encode(\"Is the football season now maybe\".split()), \"Is the football season now maybe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>times</th>\n",
       "      <th>it</th>\n",
       "      <th>the</th>\n",
       "      <th>best</th>\n",
       "      <th>was</th>\n",
       "      <th>of</th>\n",
       "      <th>worst</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>It</th>\n",
       "      <th>wisdom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  times  it  the  best  was  of  worst  foolishness  It  wisdom\n",
       "0    0      1   0    1     1    1   1      0            0   1       0\n",
       "1    0      1   1    1     0    1   1      1            0   0       0\n",
       "2    1      0   1    1     0    1   1      0            0   0       1\n",
       "3    1      0   1    1     0    1   1      0            1   0       0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(onehot, columns=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# First two sentences\n",
    "sim = [onehot[0][i] & onehot[1][i] for i in range(0, len(vocabulary))]\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(sum(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar dot product way\n",
    "import numpy as np\n",
    "np.dot(onehot[0], onehot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6, 4, 3, 3],\n",
       "        [4, 6, 4, 4],\n",
       "        [3, 4, 6, 5],\n",
       "        [3, 4, 5, 6]]),\n",
       " ['It was the best of times',\n",
       "  'it was the worst of times',\n",
       "  'it was the age of wisdom',\n",
       "  'it was the age of foolishness'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity Matrix \n",
    "np.dot(onehot, np.transpose(onehot)), sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diagnoal above is itself which you can see by:\n",
    "np.dot(onehot[0], onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing this encoding in scikit-learn with multiple words like here\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "lb = MultiLabelBinarizer()\n",
    "lb.fit([vocabulary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.transform(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_sentences = sentences + [\"John likes to watch movies. Mary likes movies too.\",\n",
    "                             \"Mary also likes to watch football games.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(more_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "               encoding='utf-8', input='content', lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "               ngram_range=(1,1), preprocessor=None, stop_words=None, strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "               tokenizer=None, vocabulary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'also', 'best', 'foolishness', 'football', 'games', 'it', 'john', 'likes', 'mary', 'movies', 'of', 'the', 'times', 'to', 'too', 'was', 'watch', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming documents to vectors\n",
    "dt = cv.transform(more_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 38 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>also</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>it</th>\n",
       "      <th>john</th>\n",
       "      <th>likes</th>\n",
       "      <th>mary</th>\n",
       "      <th>movies</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>times</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>was</th>\n",
       "      <th>watch</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  also  best  foolishness  football  games  it  john  likes  mary  \\\n",
       "0    0     0     1            0         0      0   1     0      0     0   \n",
       "1    0     0     0            0         0      0   1     0      0     0   \n",
       "2    1     0     0            0         0      0   1     0      0     0   \n",
       "3    1     0     0            1         0      0   1     0      0     0   \n",
       "4    0     0     0            0         0      0   0     1      2     1   \n",
       "5    0     1     0            0         1      1   0     0      1     1   \n",
       "\n",
       "   movies  of  the  times  to  too  was  watch  wisdom  worst  \n",
       "0       0   1    1      1   0    0    1      0       0      0  \n",
       "1       0   1    1      1   0    0    1      0       0      1  \n",
       "2       0   1    1      0   0    0    1      0       1      0  \n",
       "3       0   1    1      0   0    0    1      0       0      0  \n",
       "4       2   0    0      0   1    1    0      1       0      0  \n",
       "5       0   0    0      0   1    0    0      1       0      0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dt.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83333333]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate similarities, dot product no longer works when length varies\n",
    "# Euclidean distance fails as dimensions increase\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(dt[0], dt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.524142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524142</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  1.000000  0.833333  0.666667  0.666667  0.000000  0.000000\n",
       "1  0.833333  1.000000  0.666667  0.666667  0.000000  0.000000\n",
       "2  0.666667  0.666667  1.000000  0.833333  0.000000  0.000000\n",
       "3  0.666667  0.666667  0.833333  1.000000  0.000000  0.000000\n",
       "4  0.000000  0.000000  0.000000  0.000000  1.000000  0.524142\n",
       "5  0.000000  0.000000  0.000000  0.000000  0.524142  1.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cosine_similarity(dt, dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_dt = tfidf.fit_transform(dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>also</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>football</th>\n",
       "      <th>games</th>\n",
       "      <th>it</th>\n",
       "      <th>john</th>\n",
       "      <th>likes</th>\n",
       "      <th>mary</th>\n",
       "      <th>movies</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>times</th>\n",
       "      <th>to</th>\n",
       "      <th>too</th>\n",
       "      <th>was</th>\n",
       "      <th>watch</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.56978</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.467228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.467228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.56978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.467228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.56978</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.467228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.56978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305609</td>\n",
       "      <td>0.501208</td>\n",
       "      <td>0.250604</td>\n",
       "      <td>0.611219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250604</td>\n",
       "      <td>0.305609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250604</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419233</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.419233</td>\n",
       "      <td>0.419233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343777</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age      also     best  foolishness  football     games        it  \\\n",
       "0  0.000000  0.000000  0.56978      0.00000  0.000000  0.000000  0.338027   \n",
       "1  0.000000  0.000000  0.00000      0.00000  0.000000  0.000000  0.338027   \n",
       "2  0.467228  0.000000  0.00000      0.00000  0.000000  0.000000  0.338027   \n",
       "3  0.467228  0.000000  0.00000      0.56978  0.000000  0.000000  0.338027   \n",
       "4  0.000000  0.000000  0.00000      0.00000  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.419233  0.00000      0.00000  0.419233  0.419233  0.000000   \n",
       "\n",
       "       john     likes      mary    movies        of       the     times  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.338027  0.338027  0.467228   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.338027  0.338027  0.467228   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.338027  0.338027  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.338027  0.338027  0.000000   \n",
       "4  0.305609  0.501208  0.250604  0.611219  0.000000  0.000000  0.000000   \n",
       "5  0.000000  0.343777  0.343777  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         to       too       was     watch   wisdom    worst  \n",
       "0  0.000000  0.000000  0.338027  0.000000  0.00000  0.00000  \n",
       "1  0.000000  0.000000  0.338027  0.000000  0.00000  0.56978  \n",
       "2  0.000000  0.000000  0.338027  0.000000  0.56978  0.00000  \n",
       "3  0.000000  0.000000  0.338027  0.000000  0.00000  0.00000  \n",
       "4  0.250604  0.305609  0.000000  0.250604  0.00000  0.00000  \n",
       "5  0.343777  0.000000  0.000000  0.343777  0.00000  0.00000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names()) # notice scaled down for 'it' but not for 'wisdom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675351</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.675351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.675351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.675351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.43076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.43076</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3        4        5\n",
       "0  1.000000  0.675351  0.457049  0.457049  0.00000  0.00000\n",
       "1  0.675351  1.000000  0.457049  0.457049  0.00000  0.00000\n",
       "2  0.457049  0.457049  1.000000  0.675351  0.00000  0.00000\n",
       "3  0.457049  0.457049  0.675351  1.000000  0.00000  0.00000\n",
       "4  0.000000  0.000000  0.000000  0.000000  1.00000  0.43076\n",
       "5  0.000000  0.000000  0.000000  0.000000  0.43076  1.00000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cosine_similarity(tfidf_dt, tfidf_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ABC Dataset from Kaggle -- https://www.kaggle.com/therohk/million-headlines/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publish_date                                      headline_text\n",
       "0   2003-02-19  aba decides against community broadcasting lic...\n",
       "1   2003-02-19     act fire witnesses must be aware of defamation\n",
       "2   2003-02-19     a g calls for infrastructure protection summit\n",
       "3   2003-02-19           air nz staff in aust strike for pay rise\n",
       "4   2003-02-19      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = pd.read_csv('abcnews-date-text.csv', parse_dates=['publish_date'])\n",
    "print(len(headlines))\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publish_date,headline_text\r\n",
      "20030219,aba decides against community broadcasting licence\r\n",
      "20030219,act fire witnesses must be aware of defamation\r\n",
      "20030219,a g calls for infrastructure protection summit\r\n",
      "20030219,air nz staff in aust strike for pay rise\r\n",
      "20030219,air nz strike to affect australian travellers\r\n",
      "20030219,ambitious olsson wins triple jump\r\n",
      "20030219,antic delighted with record breaking barca\r\n",
      "20030219,aussie qualifier stosur wastes four memphis match\r\n",
      "20030219,aust addresses un security council over iraq\r\n"
     ]
    }
   ],
   "source": [
    "!head abcnews-date-text.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create tfidf vector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1226258x104691 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7933451 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.56 ms, sys: 3.28 ms, total: 8.84 ms\n",
      "Wall time: 50 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cosine_similarity(dt[0:1000], dt[0:1000]) # doing for 1000 since that'll run faster than 10k in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if not installed already\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "# Stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "print(len(stopwords))\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1226258x104411 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6394835 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19.39380478936594, '%')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % reduced\n",
    "(7933451-6394853)/(7933451.0)*100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1226258x63385 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6353809 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words have to appear at least twice\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=2)\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1226258x6921 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5447786 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also filter via word appearing certain %\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=0.0001)\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1226258x104411 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6394835 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also eliminating words that appear too frequently -- like 10%\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, max_df=0.1)\n",
    "dt = tfidf.fit_transform(headlines['headline_text'])\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if below cell doesn't work\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic analysis\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nouns_adjectives_verbs = [\"NOUN\", \"PROPN\", \"ADJ\", \"ADV\", \"VERB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publish_date                                      headline_text\n",
       "0   2003-02-19  aba decides against community broadcasting lic...\n",
       "1   2003-02-19     act fire witnesses must be aware of defamation\n",
       "2   2003-02-19     a g calls for infrastructure protection summit\n",
       "3   2003-02-19           air nz staff in aust strike for pay rise\n",
       "4   2003-02-19      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_5000 = headlines[0:5000]\n",
    "headlines_5000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "# This takes extremely long so doing for only 5000\n",
    "for i, row in headlines_5000.iterrows():\n",
    "    doc = nlp(str(row[\"headline_text\"]))\n",
    "    headlines_5000.at[i, \"lemmas\"] = \" \".join([token.lemma_ for token in doc])\n",
    "    headlines_5000.at[i, \"nav\"] = \" \".join([token.lemma_ for token in doc if token.pos_ in nouns_adjectives_verbs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>nav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>aba decide against community broadcasting licence</td>\n",
       "      <td>aba decide community broadcasting licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>act fire witness must be aware of defamation</td>\n",
       "      <td>act fire witness be aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>a g call for infrastructure protection summit</td>\n",
       "      <td>g call infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>air nz staff aust strike pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>air nz strike to affect australian traveller</td>\n",
       "      <td>air nz strike affect australian traveller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>2003-03-14</td>\n",
       "      <td>slater stars for blues on day one</td>\n",
       "      <td>slater star for blue on day one</td>\n",
       "      <td>slater star blue day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>2003-03-14</td>\n",
       "      <td>sopranos filming delayed over contract dispute</td>\n",
       "      <td>sopranos filming delay over contract dispute</td>\n",
       "      <td>sopranos filming delay contract dispute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>2003-03-14</td>\n",
       "      <td>souris outlines regional roads funding</td>\n",
       "      <td>souris outline regional road fund</td>\n",
       "      <td>souris outline regional road fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2003-03-14</td>\n",
       "      <td>south east water licensees to pay levy</td>\n",
       "      <td>south east water licensees to pay levy</td>\n",
       "      <td>south east water licensees pay levy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>2003-03-14</td>\n",
       "      <td>sri lanka hoping for new zealand defeat</td>\n",
       "      <td>sri lanka hope for new zealand defeat</td>\n",
       "      <td>sri lanka hope new zealand defeat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     publish_date                                      headline_text  \\\n",
       "0      2003-02-19  aba decides against community broadcasting lic...   \n",
       "1      2003-02-19     act fire witnesses must be aware of defamation   \n",
       "2      2003-02-19     a g calls for infrastructure protection summit   \n",
       "3      2003-02-19           air nz staff in aust strike for pay rise   \n",
       "4      2003-02-19      air nz strike to affect australian travellers   \n",
       "...           ...                                                ...   \n",
       "4995   2003-03-14                  slater stars for blues on day one   \n",
       "4996   2003-03-14     sopranos filming delayed over contract dispute   \n",
       "4997   2003-03-14             souris outlines regional roads funding   \n",
       "4998   2003-03-14             south east water licensees to pay levy   \n",
       "4999   2003-03-14            sri lanka hoping for new zealand defeat   \n",
       "\n",
       "                                                 lemmas  \\\n",
       "0     aba decide against community broadcasting licence   \n",
       "1          act fire witness must be aware of defamation   \n",
       "2         a g call for infrastructure protection summit   \n",
       "3              air nz staff in aust strike for pay rise   \n",
       "4          air nz strike to affect australian traveller   \n",
       "...                                                 ...   \n",
       "4995                    slater star for blue on day one   \n",
       "4996       sopranos filming delay over contract dispute   \n",
       "4997                  souris outline regional road fund   \n",
       "4998             south east water licensees to pay levy   \n",
       "4999              sri lanka hope for new zealand defeat   \n",
       "\n",
       "                                            nav  \n",
       "0     aba decide community broadcasting licence  \n",
       "1          act fire witness be aware defamation  \n",
       "2       g call infrastructure protection summit  \n",
       "3             air nz staff aust strike pay rise  \n",
       "4     air nz strike affect australian traveller  \n",
       "...                                         ...  \n",
       "4995                       slater star blue day  \n",
       "4996    sopranos filming delay contract dispute  \n",
       "4997          souris outline regional road fund  \n",
       "4998        south east water licensees pay levy  \n",
       "4999          sri lanka hope new zealand defeat  \n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5000x5569 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24853 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_5000 = TfidfVectorizer(stop_words=stopwords)\n",
    "dt_5000 = tfidf_5000.fit_transform(headlines_5000[\"lemmas\"].map(str))\n",
    "dt_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>10000</th>\n",
       "      <th>100000</th>\n",
       "      <th>100th</th>\n",
       "      <th>1035</th>\n",
       "      <th>106</th>\n",
       "      <th>108</th>\n",
       "      <th>11</th>\n",
       "      <th>110</th>\n",
       "      <th>...</th>\n",
       "      <th>zanetti</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zellweger</th>\n",
       "      <th>ziege</th>\n",
       "      <th>zim</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  5569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       10  100  10000  100000  100th  1035  106  108   11  110  ...  zanetti  \\\n",
       "0     0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "1     0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "2     0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "3     0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "4     0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "...   ...  ...    ...     ...    ...   ...  ...  ...  ...  ...  ...      ...   \n",
       "4995  0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "4996  0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "4997  0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "4998  0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "4999  0.0  0.0    0.0     0.0    0.0   0.0  0.0  0.0  0.0  0.0  ...      0.0   \n",
       "\n",
       "       zealand  zellweger  ziege  zim  zimbabwe  zimbabwean  zimmerman  zone  \\\n",
       "0     0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "1     0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "2     0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "3     0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "4     0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "...        ...        ...    ...  ...       ...         ...        ...   ...   \n",
       "4995  0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "4996  0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "4997  0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "4998  0.000000        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "4999  0.444635        0.0    0.0  0.0       0.0         0.0        0.0   0.0   \n",
       "\n",
       "      zoning  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "...      ...  \n",
       "4995     0.0  \n",
       "4996     0.0  \n",
       "4997     0.0  \n",
       "4998     0.0  \n",
       "4999     0.0  \n",
       "\n",
       "[5000 rows x 5569 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dt_5000.toarray(), columns=tfidf_5000.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5000x5435 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24321 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_5000 = TfidfVectorizer(stop_words=stopwords)\n",
    "dt_5000 = tfidf_5000.fit_transform(headlines_5000[\"nav\"].map(str))\n",
    "dt_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10000 = pd.read_csv(\"https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x2692 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5722 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_5000 = TfidfVectorizer(stop_words=set(top_10000.iloc[:,0].values))\n",
    "dt_5000 = tfidf_5000.fit_transform(headlines_5000[\"nav\"].map(str))\n",
    "dt_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.47300686649398, '%')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % change\n",
    "(24321-5722)/(24321.0)*100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1226258, 629481)\n",
      "76644720\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2), min_df=2)\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
    "print(dt.shape)\n",
    "print(dt.data.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1226258, 845569)\n",
      "82521160\n"
     ]
    }
   ],
   "source": [
    "# Tri-gram\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3), min_df=2)\n",
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
    "print(dt.shape)\n",
    "print(dt.data.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x4697 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7829 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_5000 = TfidfVectorizer(ngram_range=(1,2), stop_words=set(top_10000.iloc[:,0].values))\n",
    "dt_5000 = tfidf_5000.fit_transform(headlines_5000[\"nav\"].map(str))\n",
    "dt_5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.add(\"test\") # test headlines\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2), min_df=2,\n",
    "                       norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "dt = tfidf.fit_transform(headlines[\"headline_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_up = tfidf.transform([\"austrailia and new zealand discuss optimal apple size\"])\n",
    "\n",
    "# cosine similarity\n",
    "sim = cosine_similarity(made_up, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publish_date     2015-06-04 00:00:00\n",
       "headline_text       new zealand wool\n",
       "Name: 956081, dtype: object"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.iloc[np.argmax(sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publish_date       2008-10-16 00:00:00\n",
       "headline_text    uk unemployment soars\n",
       "Name: 417655, dtype: object"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "made_up_2 = tfidf.transform([\"Gas prices rising and unemployment soars high\"])\n",
    "\n",
    "sim_2 = cosine_similarity(made_up_2, dt)\n",
    "headlines.iloc[np.argmax(sim_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 ms, sys: 19.9 ms, total: 48.9 ms\n",
      "Wall time: 47.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<10000x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1818430 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Finding similar headlines\n",
    "np.dot(dt[0:10000], np.transpose(dt[0:10000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226258"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 11s, sys: 14.2 s, total: 9min 26s\n",
      "Wall time: 9min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch = 10000\n",
    "max_sim = 0.0\n",
    "max_a = None\n",
    "max_b = None\n",
    "\n",
    "for a in range(0, dt.shape[0], batch):\n",
    "    for b in range(0, a+batch, batch):\n",
    "        r = np.dot(dt[a:a+batch],np.transpose(dt[b:b+batch]))\n",
    "        # eliminate identical vectors by setting their similarity to np.nan which gets sorted out\n",
    "        r[r > 0.9999] = np.nan\n",
    "        sim = r.max()\n",
    "        if sim > max_sim:\n",
    "            # argmax returns a single value which we have to map to the two dimensions\n",
    "            (max_a, max_b) = np.unravel_index(np.argmax(r), r.shape)\n",
    "            # adjust offsets in ocrpus (this is a submatrix)\n",
    "            max_a += a\n",
    "            max_b += b\n",
    "            max_sim = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publish_date                                2014-09-18 00:00:00\n",
      "headline_text    vline fails to meet punctuality targets report\n",
      "Name: 903760, dtype: object\n",
      "publish_date                         2008-02-15 00:00:00\n",
      "headline_text    vline fails to meet punctuality targets\n",
      "Name: 364007, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(headlines.iloc[max_a])\n",
    "print(headlines.iloc[max_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_word = TfidfVectorizer(stop_words=stopwords, min_df=1000)\n",
    "dt_word = tfidf_word.fit_transform(headlines[\"headline_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vocabulary similarity\n",
    "r = cosine_similarity(dt_word.T, dt_word.T)\n",
    "np.fill_diagonal(r,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.00554026, 0.00074709, ..., 0.0013774 , 0.00181772,\n",
       "        0.00531874],\n",
       "       [0.00554026, 0.        , 0.00068462, ..., 0.00113481, 0.00167518,\n",
       "        0.0046172 ],\n",
       "       [0.00074709, 0.00068462, 0.        , ..., 0.00036739, 0.        ,\n",
       "        0.00209499],\n",
       "       ...,\n",
       "       [0.0013774 , 0.00113481, 0.00036739, ..., 0.        , 0.00347424,\n",
       "        0.        ],\n",
       "       [0.00181772, 0.00167518, 0.        , ..., 0.00347424, 0.        ,\n",
       "        0.        ],\n",
       "       [0.00531874, 0.0046172 , 0.00209499, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = tfidf_word.get_feature_names()\n",
    "size = r.shape[0] # quadratic\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '100',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '30',\n",
       " '50',\n",
       " 'abbott',\n",
       " 'abc',\n",
       " 'aboriginal',\n",
       " 'abuse',\n",
       " 'accc',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accused',\n",
       " 'act',\n",
       " 'action',\n",
       " 'address',\n",
       " 'adelaide',\n",
       " 'admits',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afl',\n",
       " 'africa',\n",
       " 'aged',\n",
       " 'agreement',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aims',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'al',\n",
       " 'alcohol',\n",
       " 'alert',\n",
       " 'alice',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'alp',\n",
       " 'ambulance',\n",
       " 'amid',\n",
       " 'analysis',\n",
       " 'andrew',\n",
       " 'anger',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'anniversary',\n",
       " 'announces',\n",
       " 'anti',\n",
       " 'anzac',\n",
       " 'apologises',\n",
       " 'appeal',\n",
       " 'approval',\n",
       " 'april',\n",
       " 'armed',\n",
       " 'army',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrests',\n",
       " 'art',\n",
       " 'asbestos',\n",
       " 'ashes',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'asked',\n",
       " 'assault',\n",
       " 'asylum',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'attempted',\n",
       " 'august',\n",
       " 'aussie',\n",
       " 'aussies',\n",
       " 'aust',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'australians',\n",
       " 'australias',\n",
       " 'authorities',\n",
       " 'award',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'baby',\n",
       " 'backs',\n",
       " 'bad',\n",
       " 'baghdad',\n",
       " 'bail',\n",
       " 'bali',\n",
       " 'ban',\n",
       " 'bangladesh',\n",
       " 'bank',\n",
       " 'banks',\n",
       " 'banned',\n",
       " 'bans',\n",
       " 'bashing',\n",
       " 'basin',\n",
       " 'battle',\n",
       " 'bay',\n",
       " 'beach',\n",
       " 'beat',\n",
       " 'beats',\n",
       " 'beattie',\n",
       " 'beef',\n",
       " 'begin',\n",
       " 'begins',\n",
       " 'bendigo',\n",
       " 'benefits',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bhp',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'bike',\n",
       " 'bikie',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'bird',\n",
       " 'black',\n",
       " 'blamed',\n",
       " 'blast',\n",
       " 'blasts',\n",
       " 'blaze',\n",
       " 'blood',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'body',\n",
       " 'bomb',\n",
       " 'bombers',\n",
       " 'bombing',\n",
       " 'book',\n",
       " 'boom',\n",
       " 'boost',\n",
       " 'boosts',\n",
       " 'border',\n",
       " 'boss',\n",
       " 'boy',\n",
       " 'boys',\n",
       " 'brawl',\n",
       " 'break',\n",
       " 'breaks',\n",
       " 'bridge',\n",
       " 'bring',\n",
       " 'brings',\n",
       " 'brisbane',\n",
       " 'british',\n",
       " 'broken',\n",
       " 'broncos',\n",
       " 'broome',\n",
       " 'brown',\n",
       " 'budget',\n",
       " 'building',\n",
       " 'bulldogs',\n",
       " 'bulls',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'bushfire',\n",
       " 'bushfires',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'cabinet',\n",
       " 'cairns',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'canberra',\n",
       " 'cancer',\n",
       " 'candidate',\n",
       " 'cane',\n",
       " 'cannabis',\n",
       " 'capital',\n",
       " 'captain',\n",
       " 'car',\n",
       " 'carbon',\n",
       " 'care',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cash',\n",
       " 'cats',\n",
       " 'cattle',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'cbd',\n",
       " 'celebrates',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'ceo',\n",
       " 'challenge',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changes',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'charges',\n",
       " 'charity',\n",
       " 'chase',\n",
       " 'check',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'children',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chopper',\n",
       " 'chris',\n",
       " 'christmas',\n",
       " 'church',\n",
       " 'city',\n",
       " 'claim',\n",
       " 'claims',\n",
       " 'clarke',\n",
       " 'clash',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'cleared',\n",
       " 'climate',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closer',\n",
       " 'closes',\n",
       " 'closure',\n",
       " 'club',\n",
       " 'coach',\n",
       " 'coal',\n",
       " 'coalition',\n",
       " 'coast',\n",
       " 'cold',\n",
       " 'collapse',\n",
       " 'come',\n",
       " 'comments',\n",
       " 'commission',\n",
       " 'commissioner',\n",
       " 'committee',\n",
       " 'commonwealth',\n",
       " 'communities',\n",
       " 'community',\n",
       " 'company',\n",
       " 'compensation',\n",
       " 'compo',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'concerns',\n",
       " 'conditions',\n",
       " 'conference',\n",
       " 'confidence',\n",
       " 'confident',\n",
       " 'confirms',\n",
       " 'consider',\n",
       " 'considers',\n",
       " 'construction',\n",
       " 'continue',\n",
       " 'continues',\n",
       " 'contract',\n",
       " 'control',\n",
       " 'coronavirus',\n",
       " 'coroner',\n",
       " 'corruption',\n",
       " 'cost',\n",
       " 'costello',\n",
       " 'costs',\n",
       " 'council',\n",
       " 'councillor',\n",
       " 'councils',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'court',\n",
       " 'covid',\n",
       " 'cowboys',\n",
       " 'crackdown',\n",
       " 'crash',\n",
       " 'crashes',\n",
       " 'creek',\n",
       " 'crews',\n",
       " 'cricket',\n",
       " 'crime',\n",
       " 'crisis',\n",
       " 'critical',\n",
       " 'criticises',\n",
       " 'criticism',\n",
       " 'crop',\n",
       " 'cross',\n",
       " 'crows',\n",
       " 'cup',\n",
       " 'custody',\n",
       " 'cut',\n",
       " 'cuts',\n",
       " 'cyclone',\n",
       " 'dairy',\n",
       " 'dam',\n",
       " 'damage',\n",
       " 'daniel',\n",
       " 'darling',\n",
       " 'darwin',\n",
       " 'data',\n",
       " 'date',\n",
       " 'david',\n",
       " 'day',\n",
       " 'days',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deadly',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'deaths',\n",
       " 'debate',\n",
       " 'debt',\n",
       " 'decision',\n",
       " 'defence',\n",
       " 'defends',\n",
       " 'delay',\n",
       " 'delayed',\n",
       " 'delays',\n",
       " 'demand',\n",
       " 'demands',\n",
       " 'denies',\n",
       " 'despite',\n",
       " 'details',\n",
       " 'detention',\n",
       " 'development',\n",
       " 'die',\n",
       " 'dies',\n",
       " 'director',\n",
       " 'disability',\n",
       " 'disaster',\n",
       " 'discuss',\n",
       " 'discusses',\n",
       " 'disease',\n",
       " 'dismisses',\n",
       " 'dispute',\n",
       " 'doctor',\n",
       " 'doctors',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'dollar',\n",
       " 'domestic',\n",
       " 'donald',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'dr',\n",
       " 'draw',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drivers',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'drops',\n",
       " 'drought',\n",
       " 'drug',\n",
       " 'drugs',\n",
       " 'drum',\n",
       " 'dry',\n",
       " 'dump',\n",
       " 'eagles',\n",
       " 'early',\n",
       " 'ease',\n",
       " 'east',\n",
       " 'easter',\n",
       " 'economic',\n",
       " 'economy',\n",
       " 'education',\n",
       " 'efforts',\n",
       " 'elderly',\n",
       " 'election',\n",
       " 'emergency',\n",
       " 'end',\n",
       " 'ends',\n",
       " 'energy',\n",
       " 'england',\n",
       " 'environment',\n",
       " 'escape',\n",
       " 'eu',\n",
       " 'evidence',\n",
       " 'ex',\n",
       " 'exchange',\n",
       " 'expansion',\n",
       " 'expected',\n",
       " 'expert',\n",
       " 'experts',\n",
       " 'explosion',\n",
       " 'export',\n",
       " 'exports',\n",
       " 'extended',\n",
       " 'extra',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'faces',\n",
       " 'facing',\n",
       " 'factory',\n",
       " 'fails',\n",
       " 'fall',\n",
       " 'falls',\n",
       " 'families',\n",
       " 'family',\n",
       " 'fans',\n",
       " 'far',\n",
       " 'farm',\n",
       " 'farmer',\n",
       " 'farmers',\n",
       " 'farming',\n",
       " 'fast',\n",
       " 'fatal',\n",
       " 'father',\n",
       " 'fear',\n",
       " 'fears',\n",
       " 'fed',\n",
       " 'federal',\n",
       " 'federer',\n",
       " 'female',\n",
       " 'festival',\n",
       " 'fight',\n",
       " 'fighting',\n",
       " 'figures',\n",
       " 'fiji',\n",
       " 'film',\n",
       " 'final',\n",
       " 'finals',\n",
       " 'finance',\n",
       " 'financial',\n",
       " 'find',\n",
       " 'finds',\n",
       " 'fine',\n",
       " 'fined',\n",
       " 'fire',\n",
       " 'firefighters',\n",
       " 'fires',\n",
       " 'firm',\n",
       " 'fish',\n",
       " 'fishing',\n",
       " 'fix',\n",
       " 'flight',\n",
       " 'flights',\n",
       " 'flood',\n",
       " 'flooding',\n",
       " 'floods',\n",
       " 'flu',\n",
       " 'fly',\n",
       " 'flying',\n",
       " 'focus',\n",
       " 'food',\n",
       " 'footage',\n",
       " 'football',\n",
       " 'force',\n",
       " 'forced',\n",
       " 'forces',\n",
       " 'foreign',\n",
       " 'forest',\n",
       " 'forestry',\n",
       " 'form',\n",
       " 'forum',\n",
       " 'found',\n",
       " 'france',\n",
       " 'fraser',\n",
       " 'fraud',\n",
       " 'free',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'fruit',\n",
       " 'fuel',\n",
       " 'fund',\n",
       " 'funding',\n",
       " 'funds',\n",
       " 'funeral',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gas',\n",
       " 'gay',\n",
       " 'gaza',\n",
       " 'general',\n",
       " 'george',\n",
       " 'german',\n",
       " 'gets',\n",
       " 'giant',\n",
       " 'gillard',\n",
       " 'gippsland',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'global',\n",
       " 'gm',\n",
       " 'goes',\n",
       " 'gold',\n",
       " 'good',\n",
       " 'government',\n",
       " 'govt',\n",
       " 'grain',\n",
       " 'grand',\n",
       " 'grandstand',\n",
       " 'grant',\n",
       " 'granted',\n",
       " 'great',\n",
       " 'green',\n",
       " 'greens',\n",
       " 'group',\n",
       " 'groups',\n",
       " 'growers',\n",
       " 'growing',\n",
       " 'growth',\n",
       " 'guilty',\n",
       " 'gun',\n",
       " 'half',\n",
       " 'hall',\n",
       " 'hand',\n",
       " 'happy',\n",
       " 'harbour',\n",
       " 'hard',\n",
       " 'harvest',\n",
       " 'hawks',\n",
       " 'head',\n",
       " 'heads',\n",
       " 'health',\n",
       " 'hearing',\n",
       " 'hears',\n",
       " 'heart',\n",
       " 'heat',\n",
       " 'heavy',\n",
       " 'held',\n",
       " 'help',\n",
       " 'helping',\n",
       " 'helps',\n",
       " 'heritage',\n",
       " 'hewitt',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highlights',\n",
       " 'highway',\n",
       " 'hill',\n",
       " 'historic',\n",
       " 'history',\n",
       " 'hit',\n",
       " 'hits',\n",
       " 'hobart',\n",
       " 'hold',\n",
       " 'holds',\n",
       " 'holiday',\n",
       " 'home',\n",
       " 'homeless',\n",
       " 'homes',\n",
       " 'hong',\n",
       " 'honour',\n",
       " 'honours',\n",
       " 'hope',\n",
       " 'hopes',\n",
       " 'horse',\n",
       " 'hospital',\n",
       " 'hospitals',\n",
       " 'host',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'housing',\n",
       " 'howard',\n",
       " 'human',\n",
       " 'hundreds',\n",
       " 'hunt',\n",
       " 'hunter',\n",
       " 'hurt',\n",
       " 'ice',\n",
       " 'illegal',\n",
       " 'impact',\n",
       " 'improve',\n",
       " 'incident',\n",
       " 'increase',\n",
       " 'india',\n",
       " 'indian',\n",
       " 'indigenous',\n",
       " 'indonesia',\n",
       " 'indonesian',\n",
       " 'industrial',\n",
       " 'industry',\n",
       " 'infrastructure',\n",
       " 'injured',\n",
       " 'injury',\n",
       " 'inquest',\n",
       " 'inquiry',\n",
       " 'insurance',\n",
       " 'interest',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'investigate',\n",
       " 'investigation',\n",
       " 'investment',\n",
       " 'ir',\n",
       " 'iran',\n",
       " 'iraq',\n",
       " 'iraqi',\n",
       " 'iron',\n",
       " 'islamic',\n",
       " 'island',\n",
       " 'islands',\n",
       " 'israel',\n",
       " 'israeli',\n",
       " 'issue',\n",
       " 'issues',\n",
       " 'jail',\n",
       " 'jailed',\n",
       " 'james',\n",
       " 'japan',\n",
       " 'japanese',\n",
       " 'jets',\n",
       " 'job',\n",
       " 'jobs',\n",
       " 'john',\n",
       " 'johnson',\n",
       " 'join',\n",
       " 'joins',\n",
       " 'jones',\n",
       " 'judge',\n",
       " 'july',\n",
       " 'june',\n",
       " 'jury',\n",
       " 'justice',\n",
       " 'keen',\n",
       " 'keeps',\n",
       " 'key',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'killer',\n",
       " 'killing',\n",
       " 'kills',\n",
       " 'kimberley',\n",
       " 'king',\n",
       " 'knights',\n",
       " 'kong',\n",
       " 'korea',\n",
       " 'korean',\n",
       " 'labor',\n",
       " 'lake',\n",
       " 'land',\n",
       " 'lanka',\n",
       " 'late',\n",
       " 'latest',\n",
       " 'launch',\n",
       " 'launches',\n",
       " 'law',\n",
       " 'laws',\n",
       " 'lawyer',\n",
       " 'lead',\n",
       " 'leader',\n",
       " 'leaders',\n",
       " 'leadership',\n",
       " 'leads',\n",
       " 'league',\n",
       " 'leak',\n",
       " 'leave',\n",
       " 'leaves',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'level',\n",
       " 'levels',\n",
       " 'levy',\n",
       " 'liberal',\n",
       " 'liberals',\n",
       " 'licence',\n",
       " 'life',\n",
       " 'lift',\n",
       " 'light',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'line',\n",
       " 'link',\n",
       " 'linked',\n",
       " 'lions',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'local',\n",
       " 'london',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'looms',\n",
       " 'lose',\n",
       " 'loses',\n",
       " 'loss',\n",
       " 'losses',\n",
       " 'lost',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'mackay',\n",
       " 'macquarie',\n",
       " 'major',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'mans',\n",
       " 'march',\n",
       " 'marine',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'markets',\n",
       " 'marriage',\n",
       " 'martin',\n",
       " 'mass',\n",
       " 'match',\n",
       " 'mayor',\n",
       " 'meat',\n",
       " 'media',\n",
       " 'medical',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'melbourne',\n",
       " 'memorial',\n",
       " 'men',\n",
       " 'mental',\n",
       " 'merger',\n",
       " 'michael',\n",
       " 'mid',\n",
       " 'military',\n",
       " 'milk',\n",
       " 'mill',\n",
       " 'million',\n",
       " 'millions',\n",
       " 'miner',\n",
       " 'miners',\n",
       " 'mining',\n",
       " 'minister',\n",
       " 'ministers',\n",
       " 'miss',\n",
       " 'missing',\n",
       " 'mission',\n",
       " 'mixed',\n",
       " 'monday',\n",
       " 'money',\n",
       " 'months',\n",
       " 'morrison',\n",
       " 'mother',\n",
       " 'mount',\n",
       " 'moves',\n",
       " 'mp',\n",
       " 'mps',\n",
       " 'mt',\n",
       " 'murder',\n",
       " 'murray',\n",
       " 'museum',\n",
       " 'music',\n",
       " 'named',\n",
       " 'national',\n",
       " 'nationals',\n",
       " 'native',\n",
       " 'near',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'new',\n",
       " 'newcastle',\n",
       " 'news',\n",
       " 'night',\n",
       " 'north',\n",
       " 'northern',\n",
       " 'november',\n",
       " 'nrl',\n",
       " 'nrn',\n",
       " 'nsw',\n",
       " 'nt',\n",
       " 'nuclear',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'nurses',\n",
       " 'nz',\n",
       " 'obama',\n",
       " 'october',\n",
       " 'offer',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'officer',\n",
       " 'officers',\n",
       " 'official',\n",
       " 'officials',\n",
       " 'oil',\n",
       " 'old',\n",
       " 'olympic',\n",
       " 'olympics',\n",
       " 'online',\n",
       " 'open',\n",
       " 'opening',\n",
       " 'opens',\n",
       " 'opposition',\n",
       " 'origin',\n",
       " 'outback',\n",
       " 'outbreak',\n",
       " 'owner',\n",
       " 'owners',\n",
       " 'pacific',\n",
       " 'package',\n",
       " 'pair',\n",
       " 'pakistan',\n",
       " 'parents',\n",
       " 'paris',\n",
       " 'park',\n",
       " 'parliament',\n",
       " 'party',\n",
       " 'past',\n",
       " 'patient',\n",
       " 'patients',\n",
       " 'paul',\n",
       " 'pay',\n",
       " 'peace',\n",
       " 'people',\n",
       " 'perth',\n",
       " 'peter',\n",
       " 'petrol',\n",
       " 'philippines',\n",
       " 'phone',\n",
       " 'pilbara',\n",
       " 'pilot',\n",
       " 'pipeline',\n",
       " 'plan',\n",
       " 'plane',\n",
       " 'planned',\n",
       " 'planning',\n",
       " 'plans',\n",
       " 'plant',\n",
       " 'play',\n",
       " 'player',\n",
       " 'players',\n",
       " 'plays',\n",
       " 'plea',\n",
       " 'pleads',\n",
       " 'pledges',\n",
       " 'pm',\n",
       " 'png',\n",
       " 'point',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'politics',\n",
       " 'poll',\n",
       " 'pool',\n",
       " 'poor',\n",
       " 'population',\n",
       " 'porn',\n",
       " 'port',\n",
       " 'positive',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'power',\n",
       " 'premier',\n",
       " 'prepare',\n",
       " 'prepares',\n",
       " 'president',\n",
       " 'press',\n",
       " 'pressure',\n",
       " 'price',\n",
       " 'prices',\n",
       " 'prime',\n",
       " 'prince',\n",
       " 'prison',\n",
       " 'private',\n",
       " 'prize',\n",
       " 'probe',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'production',\n",
       " 'profit',\n",
       " 'program',\n",
       " 'project',\n",
       " 'promises',\n",
       " 'prompts',\n",
       " 'property',\n",
       " 'proposal',\n",
       " 'proposed',\n",
       " 'protect',\n",
       " 'protection',\n",
       " 'protest',\n",
       " 'protesters',\n",
       " 'protests',\n",
       " 'public',\n",
       " 'push',\n",
       " 'pushes',\n",
       " 'puts',\n",
       " 'qantas',\n",
       " 'qld',\n",
       " 'quake',\n",
       " 'quarter',\n",
       " 'queensland',\n",
       " 'question',\n",
       " 'questioned',\n",
       " 'questions',\n",
       " 'quits',\n",
       " 'race',\n",
       " 'racing',\n",
       " 'raid',\n",
       " 'raids',\n",
       " 'rail',\n",
       " 'rain',\n",
       " 'raise',\n",
       " 'raises',\n",
       " 'rally',\n",
       " 'rape',\n",
       " 'rare',\n",
       " 'rate',\n",
       " 'rates',\n",
       " 'reach',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'rebels',\n",
       " 'record',\n",
       " 'records',\n",
       " 'recovery',\n",
       " 'red',\n",
       " 'reds',\n",
       " 'reef',\n",
       " 'reform',\n",
       " 'refugee',\n",
       " 'refugees',\n",
       " 'regional',\n",
       " 'rejects',\n",
       " 'release',\n",
       " 'released',\n",
       " 'relief',\n",
       " 'remain',\n",
       " 'remains',\n",
       " 'remote',\n",
       " 'report',\n",
       " 'reporter',\n",
       " 'reports',\n",
       " 'rescue',\n",
       " 'rescued',\n",
       " 'research',\n",
       " 'researchers',\n",
       " 'residents',\n",
       " 'resigns',\n",
       " 'resources',\n",
       " 'response',\n",
       " 'restrictions',\n",
       " 'results',\n",
       " 'retirement',\n",
       " 'return',\n",
       " 'returns',\n",
       " 'revamp',\n",
       " 'reveal',\n",
       " 'reveals',\n",
       " 'review',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'rio',\n",
       " 'rise',\n",
       " 'rises',\n",
       " 'rising',\n",
       " 'risk',\n",
       " 'river',\n",
       " 'road',\n",
       " 'roads',\n",
       " 'robbery',\n",
       " 'rock',\n",
       " 'role',\n",
       " 'round',\n",
       " 'row',\n",
       " 'royal',\n",
       " 'rudd',\n",
       " 'rugby',\n",
       " 'rule',\n",
       " 'rules',\n",
       " 'ruling',\n",
       " 'run',\n",
       " 'running',\n",
       " 'rural',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 'sa',\n",
       " 'safe',\n",
       " 'safety',\n",
       " 'sale',\n",
       " 'sales',\n",
       " 'save',\n",
       " 'says',\n",
       " 'scandal',\n",
       " 'scare',\n",
       " 'scheme',\n",
       " 'school',\n",
       " 'schools',\n",
       " 'scientists',\n",
       " 'scott',\n",
       " 'sea',\n",
       " ...]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"kong\" related to \"hong\"\n",
      "\"sri\" related to \"lanka\"\n",
      "\"covid\" related to \"19\"\n",
      "\"seekers\" related to \"asylum\"\n",
      "\"springs\" related to \"alice\"\n",
      "\"trump\" related to \"donald\"\n",
      "\"hour\" related to \"country\"\n",
      "\"pleads\" related to \"guilty\"\n",
      "\"hill\" related to \"broken\"\n",
      "\"vs\" related to \"summary\"\n",
      "\"violence\" related to \"domestic\"\n",
      "\"climate\" related to \"change\"\n",
      "\"royal\" related to \"commission\"\n",
      "\"care\" related to \"aged\"\n",
      "\"driving\" related to \"drink\"\n",
      "\"gold\" related to \"coast\"\n",
      "\"wall\" related to \"street\"\n",
      "\"mental\" related to \"health\"\n",
      "\"scott\" related to \"morrison\"\n",
      "\"north\" related to \"korea\"\n"
     ]
    }
   ],
   "source": [
    "for index in np.argsort(r.flatten())[::-1][0:40]:\n",
    "    a = int(index/size)\n",
    "    b = index%size\n",
    "    if a > b: # avoid repetitions\n",
    "        print('\"%s\" related to \"%s\"' % (voc[a], voc[b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
